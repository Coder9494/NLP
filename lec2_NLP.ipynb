{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+pUjl6fbCQHf9pphU5oYJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Coder9494/NLP/blob/main/lec2_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/watch?v=29qyNyNkLHs&list=PLKnIA16_RmvZo7fp5kkIth6nRTeQQsjfX&index=2"
      ],
      "metadata": {
        "id": "sW0dfdG61VMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLP Pipeline\n",
        "\n",
        "NLP pipeline is a set of steps followed to build an end to end NLP Software. It consists of the following steps\n",
        "1. Data Acquisition\n",
        "2. Text Preparation \n",
        "  - Text Cleanup\n",
        "  - Basic Preprocessing\n",
        "  - Advance Preprocessing\n",
        "3. Feature Engineering\n",
        "4. Modelling\n",
        "  - Model Building\n",
        "  - Evaluation\n",
        "5. Deployment\n",
        "  - Deployment\n",
        "  - Monitoring\n",
        "  - Model Update\n",
        "\n",
        "-------------------------------------------------------------\n",
        "- The above pipeline is not univerasal\n",
        "- Deep Learning pipelines are slightly different\n",
        "- Pipeline is non-linear"
      ],
      "metadata": {
        "id": "eAXmO6x81YPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Acquision\n",
        "\n"
      ],
      "metadata": {
        "id": "tnEXJ86t3AVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Text Preparation\n",
        "\n",
        "1. Cleaning\n",
        "  - html/tag cleaning\n",
        "  - emoji\n",
        "  - spelling check\n",
        "  - slang acronyms\n",
        "\n",
        "2. Basic Preprocessing\n",
        "  - Basic/Fundamental Preprocessing (Tokenization)\n",
        "    - Sentence Tokenization\n",
        "    - Word Tokenization\n",
        "  - Optimal Preprocessing\n",
        "    - Stop word removal\n",
        "    - Stemming (bringing words to their root form (Ex: Dance, dancing, danced --> dance))\n",
        "    - Removing digits, punchations\n",
        "    - lower casing\n",
        "    - language detection\n",
        "\n",
        "3. Advanced Preprocessing\n",
        "  - Parts of speech (POS) tagging --> this can be done only if we do not remove stop words\n",
        "  - Parsing\n",
        "  - Coreframe resolution\n",
        "\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "eIDTcsYF9Nlx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Feature Engineering\n",
        "\n",
        "Converting text to numbers (Text Vectorization)\n",
        "1. Bag of Words\n",
        "2. Tfidf\n",
        "3. OHE\n",
        "4. word2vec\n",
        "\n",
        "-------------------------------------------------\n",
        "**ML Feature Engneering**: do feature engineering based on domain knowledge and create features\n",
        "\n",
        "Advantages: \n",
        "  - we can justify our result\n",
        "  - high model interpretebility\n",
        "\n",
        "Disadvantages:\n",
        "  - creating feature is hectic\n",
        "  - need domain knowledge\n",
        "  - its not guranteed that self generated features will help model\n",
        "\n",
        "------------------------------------------------------------\n",
        "\n",
        "**DL Feature Engineering**: After preprocessing, we can directly apply algo as DL algo will create features for itself automatically\n",
        "\n",
        "Advantages: \n",
        "  - automatic feature generation\n",
        "  \n",
        "Disadvantages:\n",
        "  - no model interpretebility\n"
      ],
      "metadata": {
        "id": "pdf7sL1n_99g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Modeling\n",
        "\n",
        "1. Applying models\n",
        "2. Evaluation\n",
        "\n",
        "------------------------------------------------------------------\n",
        "### Modeling\n",
        "1. Heuristic methods (Jugad)\n",
        "2. ML\n",
        "3. DL (we can also use Transfer Learning like BERT)\n",
        "4. Cloud APIs\n",
        "\n",
        "Approach will depend on\n",
        "- amount of data\n",
        "- Nature of problem\n",
        "\n",
        "-----------------------------------------------------------------\n",
        "### Evaluation\n",
        "Generally two types of evaluation\n",
        "1. Intrinsic Evaluation (on technical level) using metrics like confusion matrix, perplexity, etc\n",
        "2. Extrinsic Evaluation (on Business level) \n",
        "\n",
        "If Extrinsic evaluation is good then automatically Intrinsic evaluation is also good but the opposite may not be true"
      ],
      "metadata": {
        "id": "QdTV07QICMo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Deployment\n",
        "\n",
        "1. Deployment - depends how the product is going to be used\n",
        "  - API (microservice)\n",
        "  - Chatbot\n",
        "\n",
        "2. Monitoring:  generally create dashboards on intrinsic and extrinsic metrics (Key performing metrics)"
      ],
      "metadata": {
        "id": "hmcD8OwJHF2l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fhXl8a_71Xrt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}